{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsPaOMk+antKvgd7WuPWQu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeanBarnier/HAFS_Air-Sea/blob/main/getHAFSASlices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This program subsets and saves portions of output from the MOM6 model along the track of a given TC."
      ],
      "metadata": {
        "id": "LDINAnW1dduE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up environment"
      ],
      "metadata": {
        "id": "f592A_AhmfhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cfgrib"
      ],
      "metadata": {
        "id": "wDGgQBxjYusr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install aria2"
      ],
      "metadata": {
        "id": "ra6QPuljGkzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "import cfgrib\n",
        "import subprocess as sp"
      ],
      "metadata": {
        "id": "vWrr_30yZ4nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6KsU8RFVmin5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#User parameters"
      ],
      "metadata": {
        "id": "XnkUgaLnLbuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subfolder = \"RI\"\n",
        "saveLocAtm = \"drive/MyDrive/savedData/hafsaOutput/\" + subfolder\n",
        "saveLocOce = \"drive/MyDrive/savedData/mom6Output/\" + subfolder\n",
        "bucket = \"https://noaa-nws-hafs-pds.s3.amazonaws.com/hfsa/\"\n",
        "\n",
        "name = \"Milton\"\n",
        "tcNum = \"14\"\n",
        "trackType = \"\"\n",
        "\n",
        "start = dt(year=2024, month=10, day=8, hour=6)\n",
        "end = dt(year=2024, month=10, day=9, hour=0)\n",
        "\n",
        "fHourStep = 3 #Normally 3 for HAFS-A\n",
        "forecastLength = 126 #Normally 126 for HAFS-A. Changeable for testing.\n",
        "runStep = 6 #Normally 6 for HAFS-A\n",
        "\n",
        "downloadAtm = False\n",
        "downloadOce = True\n",
        "replaceExisting = False"
      ],
      "metadata": {
        "id": "5mX9LiKtLeTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atmTop = 700 # in hPa\n",
        "oceBottom = 530 # In m below surface. This was chosen to include a layer in the files that's around 529 m"
      ],
      "metadata": {
        "id": "nSPZ_yPqEe7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Determine Area and Temporal Extent"
      ],
      "metadata": {
        "id": "AHP_bcI8s-W6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get best track data and find bounds of TC track"
      ],
      "metadata": {
        "id": "f4C7bEjXNznh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bt = pd.read_csv(f\"/content/drive/MyDrive/savedData/{name}/hurdat2_\" + name + trackType + \".csv\")\n",
        "latBounds = [min(bt.lat), max(bt.lat)]\n",
        "lonBounds = [min(bt.lon), max(bt.lon)]"
      ],
      "metadata": {
        "id": "IwrrNNGhMYau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dateFormat = \"%Y-%m-%d %H:%M:%S\"\n",
        "runFormat = \"%Y%m%d%H\""
      ],
      "metadata": {
        "id": "N7EXfX8AgqHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find times needed"
      ],
      "metadata": {
        "id": "aQjv2UVg4l65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fcastTimes = {} #Key: initiation, item: valid time list\n",
        "\n",
        "initTime = start\n",
        "while initTime <= end:\n",
        "  validTime = initTime\n",
        "  fcastTimes[initTime] = []\n",
        "  fhour = 0\n",
        "\n",
        "  while fhour <= forecastLength:\n",
        "    fcastTimes[initTime].append(validTime)\n",
        "    validTime += pd.Timedelta(hours=fHourStep)\n",
        "    fhour += fHourStep\n",
        "\n",
        "  initTime += pd.Timedelta(hours=runStep)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jdMyhDj94ij0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = [fcastTimes[key] for key in fcastTimes.keys()]\n",
        "flat_list = [item for sublist in t for item in sublist]\n",
        "totalAtmData = 200 * len(flat_list)\n",
        "totalOceData = 30 * len(flat_list)\n",
        "print(f\"Approximate storage needed: {(totalAtmData+totalOceData)/1000} GB\")"
      ],
      "metadata": {
        "id": "-5GRx1XRJ_uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fcastTimes"
      ],
      "metadata": {
        "id": "MzL6BuzUwpp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieve Data"
      ],
      "metadata": {
        "id": "5MU8O99TOZme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine what files already exist"
      ],
      "metadata": {
        "id": "cxxzCBnyrdk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "existingFilesLs = !ls {saveLocAtm}\n",
        "existingFiles = [f for files in existingFilesLs for f in files.split(\"  \")]\n",
        "existingFileNames = [f.replace(\"nc\", \"grb2\").replace(\"hafsa\", \"atm\").replace(\"mom6\", \"oce\") for f in existingFiles]\n",
        "existingFileNames.sort()"
      ],
      "metadata": {
        "id": "kxLQmg3erdPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download HAFS"
      ],
      "metadata": {
        "id": "EIzilsV5vIKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for init, validList in fcastTimes.items():\n",
        "\n",
        "  if not downloadAtm: break\n",
        "\n",
        "  initDate, initHour = init.strftime(\"%Y%m%d_%H\").split(\"_\")\n",
        "\n",
        "  for valid in validList:\n",
        "\n",
        "    fhour = str(int((valid-init).total_seconds() / 3600))\n",
        "    while len(fhour) < 3: fhour = \"0\" + fhour\n",
        "\n",
        "    atmURL = bucket + initDate + \"/\" + initHour + \"/\" + tcNum + \"l.\" + initDate + initHour + \".hfsa.storm.atm.f\" + fhour + \".grb2\"\n",
        "    atmFile = \"atm_\" + initDate + initHour + \"_f\" + fhour + \".grb2\"\n",
        "    if atmFile in existingFileNames and not replaceExisting:\n",
        "      print(atmFile + \" already exists.\\n\")\n",
        "      continue #Skip files we already have\n",
        "\n",
        "    !aria2c -x 16 -s 16 --allow-overwrite=true -o {atmFile} {atmURL}\n",
        "    atmData = xr.open_dataset(atmFile, engine=\"cfgrib\", decode_timedelta=True, filter_by_keys={'stepType': 'instant', 'typeOfLevel': 'isobaricInhPa'})\n",
        "\n",
        "    pressureSlice = slice(max(atmData.isobaricInhPa.data), atmTop)\n",
        "    #Longitude in atm files are in degrees east, but are -180 - 180 in oce files. point has them from -180 - 180\n",
        "    atmSlice = atmData.sel(latitude=slice(latBounds[0], latBounds[1]), longitude=slice(360+lonBounds[0], 360+lonBounds[1]), isobaricInhPa=pressureSlice)\n",
        "\n",
        "    atmSlice.to_netcdf(saveLocAtm + \"/hafsa_\" + initDate + initHour + \"_f\" + fhour + \".nc\")\n",
        "\n",
        "    !rm {atmFile}"
      ],
      "metadata": {
        "id": "SZPGYEMDlF2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download MOM6"
      ],
      "metadata": {
        "id": "A9iKFgvmvNK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "existingFilesLs = !ls {saveLocOce}\n",
        "existingFiles = [f for files in existingFilesLs for f in files.split(\"  \")]\n",
        "existingFileNames = [f.replace(\"nc\", \"grb2\").replace(\"hafsa\", \"atm\").replace(\"mom6\", \"oce\") for f in existingFiles]\n",
        "existingFileNames.sort()"
      ],
      "metadata": {
        "id": "RkZRr8HBs2yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missingFiles = ['oce_2024100800_f000.nc', \"oce_2024100800_f126.nc\"]"
      ],
      "metadata": {
        "id": "f41oFyaQ2T4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for init, validList in fcastTimes.items():\n",
        "\n",
        "  if not downloadOce: break\n",
        "\n",
        "  initDate, initHour = init.strftime(\"%Y%m%d_%H\").split(\"_\")\n",
        "  oceRun = False\n",
        "\n",
        "  for valid in validList:\n",
        "\n",
        "    fhour = str(int((valid-init).total_seconds() / 3600))\n",
        "    while len(fhour) < 3: fhour = \"0\" + fhour\n",
        "\n",
        "    oceURL = bucket + initDate + \"/\" + initHour + \"/\" + tcNum + \"l.\" + initDate + initHour + \".hfsa.mom6.f\" + fhour + \".nc\"\n",
        "    oceFile = \"oce_\" + initDate + initHour + \"_f\" + fhour + \".nc\"\n",
        "\n",
        "    if atmFile in existingFileNames and not replaceExisting:\n",
        "      print(atmFile + \" already exists.\\n\")\n",
        "      continue #Skip files we already have\n",
        "\n",
        "    if oceFile in missingFiles: continue #This file is missing\n",
        "    !aria2c -x 16 -s 16 --allow-overwrite=true -o {oceFile} {oceURL}\n",
        "    oceData = xr.open_dataset(oceFile, decode_times=False)\n",
        "\n",
        "    depthSlice = slice(min(oceData.z_l.data), oceBottom)\n",
        "    oceSlice = oceData.sel(z_l=depthSlice, z_i=depthSlice,\\\n",
        "                           xh=slice(lonBounds[0],lonBounds[1]), yh=slice(latBounds[0],latBounds[1]),\\\n",
        "                           xq=slice(lonBounds[0],lonBounds[1]), yq=slice(latBounds[0],latBounds[1]))\n",
        "\n",
        "    oceSlice.to_netcdf(saveLocOce + \"/mom6_\" + initDate + initHour + \"_f\" + fhour + \".nc\")\n",
        "\n",
        "    !rm {oceFile}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mU1HIzL_4ij2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}